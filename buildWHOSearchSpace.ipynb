{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4d8e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4abbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48301034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# treat empty strings and np.nan as NAm will impact dropna method\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "# set Jupyter to display all output from a cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e94f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words = list(stop_words)\n",
    "stop_words.append('other')\n",
    "stop_words = set(stop_words)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f67c7",
   "metadata": {},
   "source": [
    "df_allDocDetails_with_ranking = pd.read_csv(\"results/df_allDocDetails_with_ranking.csv\")\n",
    "df_pubmed_articles = pd.read_csv(\"results/df_pubmed_articles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d214e3",
   "metadata": {},
   "source": [
    "df_allDocDetails_with_ranking.shape\n",
    "df_allDocDetails_with_ranking.columns\n",
    "df_pubmed_articles.shape\n",
    "df_pubmed_articles.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647eed5d",
   "metadata": {},
   "source": [
    "df_allDocDetails_with_ranking.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82697a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    " \n",
    "def filter_stop_words(sent):\n",
    "    word_tokens = word_tokenize(sent)\n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "    filtered_sentence = []\n",
    "\n",
    "    for w in word_tokens:\n",
    "        if (w not in stop_words) and (not re.search(r'^\\d+$', w)):\n",
    "            filtered_sentence.append(w)\n",
    "\n",
    "    return(' '.join(filtered_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c18716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_leading_spaces(sent):\n",
    "    return len(sent) - len(sent.lstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81019d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isSectionHeader(x):\n",
    "    if re.search(r'^\\s*\\d+\\s+', x):\n",
    "        x = re.sub(r'^\\s*\\d+\\s+', '', x)\n",
    "        return(1, x)\n",
    "    return(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afe719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extraneous_info(item, spaces):\n",
    "    if item['space'] in spaces:\n",
    "        item['text'] = re.sub(r',.*', '', item['text'])\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e61f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_after_with(text):\n",
    "    txt = re.sub(r'associated with.*$', '', text).strip()\n",
    "    return re.sub(r'with.*$', '', txt).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90017dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_abbrev_alternate_search_term(items, item):\n",
    "    res = re.search(r'\\(([A-Z]+)?\\)', item['text'])\n",
    "    #print(item['text'])\n",
    "    if res:\n",
    "        #print(item)\n",
    "        abbrev = res.groups(0)[0]\n",
    "        non_abbrev = ''\n",
    "        split_text = item['text'].split()\n",
    "        for ind, tok in enumerate(split_text):\n",
    "            if tok == f'({abbrev})':\n",
    "                start_letter_abbrev = abbrev[0]\n",
    "                ind_start = ind-len(abbrev)\n",
    "                while(ind_start >= 0):\n",
    "                    if split_text[ind_start][0] == start_letter_abbrev:\n",
    "                        break\n",
    "                    ind_start -= 1\n",
    "                non_abbrev = ' '.join(split_text[ind_start:ind])\n",
    "                #print(ind, tok)\n",
    "        refined_text_with_only_abbrev = re.sub(f'\\s*{non_abbrev}\\s*',' ', item['text']).strip()\n",
    "        refined_text_with_only_abbrev = re.sub(f'\\s*\\({abbrev}\\)\\s*', ' ' + abbrev + ' ', refined_text_with_only_abbrev).strip()\n",
    "        refined_text_with_only_non_abbrev = re.sub(f'\\s*\\({abbrev}\\)\\s*',' ', item['text']).strip()\n",
    "        items.append({'space':item['space'], 'text':refined_text_with_only_abbrev})\n",
    "        items.append({'space':item['space'], 'text':refined_text_with_only_non_abbrev})\n",
    "    else:\n",
    "        items.append(item)\n",
    "\n",
    "\n",
    "def extract_alternative_word_search_term(items, item):\n",
    "    txt = item['text']\n",
    "    m = re.search(r'([\\w\\-\\+]+)\\s*\\/\\s*([\\w\\-\\+]+)', txt)\n",
    "    txts = []\n",
    "    if m:\n",
    "        both_terms = m.group(0)\n",
    "        term1 = m.group(1)\n",
    "        term2 = m.group(2)\n",
    "        \n",
    "        alt1 = txt.replace(both_terms, term1)\n",
    "        alt2 = txt.replace(both_terms, term2)\n",
    "        txts.extend([alt1, alt2])\n",
    "    \n",
    "        txts = list(set(txts))\n",
    "    else:\n",
    "        txts.append(item['text'])\n",
    "    \n",
    "    for txt in txts:\n",
    "        items.append({'space': item['space'], 'text': txt})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_noun_phrases(item):\n",
    "    doc = nlp(item['text'])\n",
    "    clean_chunks = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_text = chunk.text\n",
    "        chunk_text = re.sub(r'[,;\\.\\)\\(]+', '', chunk_text)\n",
    "        chunk_text = filter_stop_words(chunk_text.lower())\n",
    "        if len(chunk_text) > 1:\n",
    "            # remove any repeated words\n",
    "            #split_chunk_text = chunk_text.split()\n",
    "            #chunk_text = ' '.join(list(set([word for word in chunk_text.split()])))\n",
    "            clean_chunks.append(chunk_text)\n",
    "    item['suggestions'] = clean_chunks\n",
    "    item['suggestions'].append(filter_stop_words(item['text'].lower()))\n",
    "    item['suggestions'] = list(set(item['suggestions']))\n",
    "    \n",
    "    # add variations to handle british vs US english such as tumour vs tumor, pediatric vs paediatric\n",
    "    variations = [suggestion.replace('paed', 'ped').replace('tumour', 'tumor') for suggestion in item['suggestions'] if re.search(r'tumour|paed', suggestion)]\n",
    "    item['suggestions'].extend(variations)\n",
    "    \n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "search_space = {}\n",
    "adult_search_space = {}\n",
    "child_search_space = {}\n",
    "JUNK_PATTERN = 'chapter|https|http|overview|iarc|all rights|Terms of use|privacy policy|copyright|BlueBooksOnline|introduction|foreword|abbreviation|references|subject|committee|declaration|contributor|volume|icd|contents|sources|contents|sources|classification|appendix'\n",
    "\n",
    "for root, dirs, files in os.walk(\"./data/who\"):\n",
    "    for filename in files:\n",
    "        if filename.endswith('.txt'):\n",
    "            my_file = open(f\"data/who/{filename}\", \"r\", encoding=\"utf-8\")\n",
    "            \n",
    "            childFile = False\n",
    "            if re.search(r'paediatric|pediatric', filename):\n",
    "                childFile = True\n",
    "\n",
    "            # reading the file\n",
    "            data = my_file.read()\n",
    "\n",
    "            # replacing end splitting the text \n",
    "            # when newline ('\\n') is seen.\n",
    "            data_into_list = data.split(\"\\n\")\n",
    "            data_into_list = [re.sub(r'–|−', '-', line) for line in data_into_list if line.strip() != '']\n",
    "            data_into_list = [re.sub(r'[^\\x20-\\x7e]', ' ', line) for line in data_into_list if line.strip() != '']\n",
    "            data_into_list = [re.sub(r'\\(\\/.*\\)$', '', line) for line in data_into_list if line.strip() != '']\n",
    "            data_into_list = [re.sub(r'\\w*\\([;\\.\\da-z]*\\d\\);*', '', line) for line in data_into_list if line.strip() != '']\n",
    "            data_into_list = [line for line in data_into_list if not re.search(JUNK_PATTERN, line, flags=re.IGNORECASE)]\n",
    "            data_into_list = [line for line in data_into_list if not re.search(r\"^[\\s,]+$\", line, flags=re.IGNORECASE) ]\n",
    "            data_into_list = [line for line in data_into_list if not re.search(r\"^[\\d\\/APM:\\s,]+$\", line, flags=re.IGNORECASE) ]\n",
    "            data_into_list = [line for line in data_into_list if not len(line.strip()) <= 1]\n",
    "            data_into_list = [line for line in data_into_list if not re.search(r'^\\(*\\s*[a-z]\\s*\\)*$', line.strip())]\n",
    "            data_into_list = [line.rstrip() for line in data_into_list]\n",
    "\n",
    "            data_into_list_with_meta = [{'space':count_leading_spaces(line), 'text':re.sub('\\s*\\d+\\.\\s*', '', line.strip())} for line in data_into_list]\n",
    "\n",
    "            refined_data_into_list_with_meta = []\n",
    "            for index, item in enumerate(data_into_list_with_meta):\n",
    "                # if starting letter is lower case then combine with previous line\n",
    "                if (len(refined_data_into_list_with_meta) > 0) and re.search('^[a-z]', item['text']):\n",
    "                    refined_data_into_list_with_meta[-1]['text'] = refined_data_into_list_with_meta[-1]['text'] + ' ' + item['text']\n",
    "                else:\n",
    "                    refined_data_into_list_with_meta.append(item)\n",
    "\n",
    "            # create alternative variations, if something is abbreviated\n",
    "            refined_refined_data_into_list_with_meta = []\n",
    "            for index, item in enumerate(refined_data_into_list_with_meta):\n",
    "                extract_abbrev_alternate_search_term(refined_refined_data_into_list_with_meta, item)\n",
    "\n",
    "\n",
    "            # handle alternatives due to presence of /\n",
    "            refined_refined_refined_data_into_list_with_meta = []\n",
    "            for index, item in enumerate(refined_refined_data_into_list_with_meta):\n",
    "                extract_alternative_word_search_term(refined_refined_refined_data_into_list_with_meta, item)\n",
    "\n",
    "            # identify the  indented space\n",
    "            spaces_list = []\n",
    "            for index, item in enumerate(refined_refined_refined_data_into_list_with_meta):\n",
    "                spaces_list.append(item['space'])\n",
    "            spaces_list = list(set(spaces_list))\n",
    "            smallest_space = min(spaces_list)\n",
    "            spaces_list.remove(smallest_space)\n",
    "            #second_largest_space = max(spaces_list)\n",
    "\n",
    "            #print(spaces_list)\n",
    "            #print(smallest_space)\n",
    "\n",
    "            refined_refined_refined_refined_data_into_list_with_meta = []\n",
    "            for index, item in enumerate(refined_refined_refined_data_into_list_with_meta):\n",
    "                refined_refined_refined_refined_data_into_list_with_meta.append(remove_extraneous_info(item, spaces_list))\n",
    "\n",
    "            # create initial search suggestions\n",
    "            refined_refined_refined_refined_data_into_list_with_meta = [extract_noun_phrases(item) for item in refined_refined_refined_refined_data_into_list_with_meta]\n",
    "            default_suggestions = refined_refined_refined_refined_data_into_list_with_meta[0]['suggestions']\n",
    "            \n",
    "            for idx, item in enumerate(refined_refined_refined_refined_data_into_list_with_meta):\n",
    "                if idx == 0:\n",
    "                    item['parent_index'] = -1\n",
    "                    item['more_suggestions'] = default_suggestions\n",
    "                elif item['space'] > refined_refined_refined_refined_data_into_list_with_meta[idx - 1]['space']:\n",
    "                    item['parent_index'] = idx - 1\n",
    "                    if item['parent_index'] == -1:\n",
    "                        item['more_suggestions'] = default_suggestions\n",
    "                    else:\n",
    "                        item['more_suggestions'] = list(set(refined_refined_refined_refined_data_into_list_with_meta[item['parent_index']]['suggestions'] + refined_refined_refined_refined_data_into_list_with_meta[item['parent_index']]['more_suggestions']))\n",
    "                elif item['space'] == refined_refined_refined_refined_data_into_list_with_meta[idx - 1]['space']:\n",
    "                    item['parent_index'] = refined_refined_refined_refined_data_into_list_with_meta[idx - 1]['parent_index']\n",
    "                    if item['parent_index'] == -1:\n",
    "                        item['more_suggestions'] = default_suggestions\n",
    "                    else:\n",
    "                        item['more_suggestions'] = list(set(refined_refined_refined_refined_data_into_list_with_meta[item['parent_index']]['suggestions'] + refined_refined_refined_refined_data_into_list_with_meta[item['parent_index']]['more_suggestions']))\n",
    "                else:\n",
    "                    current_index = refined_refined_refined_refined_data_into_list_with_meta[idx - 1]['parent_index']\n",
    "                    while(current_index >= 0):\n",
    "                        if refined_refined_refined_refined_data_into_list_with_meta[current_index]['space'] == item['space']:\n",
    "                            item['parent_index'] = refined_refined_refined_refined_data_into_list_with_meta[current_index]['parent_index']\n",
    "                            if item['parent_index'] == -1:\n",
    "                                item['more_suggestions'] = default_suggestions\n",
    "                            else:\n",
    "                                item['more_suggestions'] = list(set(refined_refined_refined_refined_data_into_list_with_meta[item['parent_index']]['suggestions'] + refined_refined_refined_refined_data_into_list_with_meta[item['parent_index']]['more_suggestions']))\n",
    "                            break\n",
    "                        elif refined_refined_refined_refined_data_into_list_with_meta[current_index]['space'] < item['space']:\n",
    "                            item['parent_index'] = current_index\n",
    "                            if item['parent_index'] == -1:\n",
    "                                item['more_suggestions'] = default_suggestions\n",
    "                            else:\n",
    "                                item['more_suggestions'] = list(set(refined_refined_refined_refined_data_into_list_with_meta[item['parent_index']]['suggestions'] + refined_refined_refined_refined_data_into_list_with_meta[item['parent_index']]['more_suggestions']))\n",
    "                            break\n",
    "                        else:\n",
    "                            current_index = refined_refined_refined_refined_data_into_list_with_meta[current_index]['parent_index']\n",
    "\n",
    "            for idx, item in enumerate(refined_refined_refined_refined_data_into_list_with_meta):\n",
    "                if idx == 0:\n",
    "                    # skip first entry\n",
    "                    continue\n",
    "                #print(idx, item)\n",
    "                #suggestions = re.split(r'\\s*,\\s*', item['suggestions'])\n",
    "                for suggestion in item['suggestions']:\n",
    "                    if suggestion in search_space:\n",
    "                        search_space[suggestion] += item['more_suggestions']\n",
    "                        search_space[suggestion] = list(set(search_space[suggestion]))\n",
    "                    else:\n",
    "                        search_space[suggestion] = item['more_suggestions']\n",
    "                        search_space[suggestion] = list(set(search_space[suggestion]))\n",
    "                        \n",
    "                    if childFile:\n",
    "                        if suggestion in child_search_space:\n",
    "                            child_search_space[suggestion] += item['more_suggestions']\n",
    "                            child_search_space[suggestion] = list(set(child_search_space[suggestion]))\n",
    "                        else:\n",
    "                            child_search_space[suggestion] = item['more_suggestions']\n",
    "                            child_search_space[suggestion] = list(set(child_search_space[suggestion]))\n",
    "                    else:\n",
    "                        if suggestion in adult_search_space:\n",
    "                            adult_search_space[suggestion] += item['more_suggestions']\n",
    "                            adult_search_space[suggestion] = list(set(adult_search_space[suggestion]))\n",
    "                        else:\n",
    "                            adult_search_space[suggestion] = item['more_suggestions']\n",
    "                            adult_search_space[suggestion] = list(set(adult_search_space[suggestion]))\n",
    "\n",
    "            my_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(search_space)\n",
    "len(child_search_space)\n",
    "len(adult_search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d40e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c03024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# create a binary pickle file \n",
    "with open('results/who_child_search_terms_mapping.pkl', 'wb') as handle:\n",
    "    pickle.dump(child_search_space, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('results/who_adult_search_terms_mapping.pkl', 'wb') as handle:\n",
    "    pickle.dump(adult_search_space, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('results/who_search_terms_mapping.pkl', 'wb') as handle:\n",
    "    pickle.dump(search_space, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd0bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/who_search_terms_mapping.pkl', 'rb') as handle:\n",
    "    out_search_space = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdc3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "\n",
    "exists('results/who_search_terms_mapping.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed642976",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_search_space['glioneuronal tumours']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e30b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(search_space.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4092452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_noun_phrases({'space':4, 'text':'Paediatric-type diffuse high-grade gliomas defined by H3 status'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5316319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set([1,2,3,4]).difference(set([2,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d250fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in search_space:\n",
    "    vals = list(set(search_space[key]))\n",
    "    vals = [re.sub(r'\\s*,\\s*|\\s*&\\s*', ' ', val) for val in vals]\n",
    "    vals = [re.sub(r'\\s+', '+', val) for val in vals]\n",
    "    print(key, '--->', vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67e1a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\n",
    "Department of Medical Oncology, UZ Brussel, Laarbeeklaan 101, 1090 Brussels, Belgium. Bart.Neyns@uzbrussel.be\n",
    "\"\"\"\n",
    "doc = nlp(txt)\n",
    "clean_chunks = []\n",
    "for chunk in doc.noun_chunks:\n",
    "    chunk_text = chunk.text\n",
    "    clean_chunks.append(chunk_text)\n",
    "print(clean_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d01b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34989fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
